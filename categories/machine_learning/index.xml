<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning on My New Hugo Site</title>
    <link>http://sjh4773.github.io/categories/machine_learning/</link>
    <description>Recent content in machine_learning on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>kr-ko</language>
    <lastBuildDate>Mon, 03 May 2021 00:29:12 +0000</lastBuildDate><atom:link href="http://sjh4773.github.io/categories/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PCA의 이해</title>
      <link>http://sjh4773.github.io/post/machine-learning-15/</link>
      <pubDate>Mon, 03 May 2021 00:29:12 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-15/</guid>
      <description>PCA(Principal Component Analysis)의 이해   고차원의 원본 데이터를 저차원의 부분 공간으로 투영하여 데이터를 축소하는 기법
  예를 들어 10차원의 데이터를 2차원의 부분 공간으로 투영하여 데이터를 축소
  PCA는 원본 데이터가 가지는 데이터 변동성을 가장 중요한 정보로 간주하며 이 변동성에 기반한 원본 데이터 투영으로 차원 축소를 수행
  PCA는 원본 데이터 변동성이 가장 큰 방향으로 순차적으로 축들을 생성하고, 이렇게 생선된 축으로 데이터를 투영하는 방식이다.
PCA는 제일 먼저 원본 데이터에 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터 축을 생성하고, 두 번째 축은 첫번째 축을 제외하고 그 다음으로 변동성이 큰 축을 설정하는데 이는 첫번째 축에 직각이 되는 벡터(직교 벡터) 축 입니다.</description>
    </item>
    
    <item>
      <title>차원 축소</title>
      <link>http://sjh4773.github.io/post/machine-learning-14/</link>
      <pubDate>Sun, 02 May 2021 23:42:56 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-14/</guid>
      <description>차원 축소 차원의 저주 차원의 저주 커질수록 데이터 포인트들 간 거리가 크게 늘어남, 데이터가 희소화(Sparse) 됨
수백~수천개 이상의 피처로 구성도니 포인트들간 거리에 기반한 ml 알고리즘이 무력화됨
또한 피처가 많을 경우 개별 피처간에 상관관계가 높아 선형 회귀와 같은 모델에서는 다중 공선성 문제로 모델의 예측 성능이 저하 될 가능성이 높음
수십 ~ 수백개의 피처들을 작은 수의 피처들로 축소한다면?  학습 데이터 크기를 줄여서 학습 시간 절약 불필요한 피처들을 줄여서 모델 성능 향상에 기여(주로 이미지 관련 데이터) 다차원의 데이터를 3차우너 이하의 차원 축소를 통해서 시각적으로 보다 쉽게 데이터 패턴 인지  피처 선택과 피처 추출 일반적으로 차춴축소는 피처 선택과 피처 추출로 나눌 수 있다.</description>
    </item>
    
    <item>
      <title>베이지안 최적화 기법 이론 보충</title>
      <link>http://sjh4773.github.io/post/machine-learning-13/</link>
      <pubDate>Thu, 29 Apr 2021 00:08:32 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-13/</guid>
      <description>베이지안 최적화 기법 이론 보충 머신러닝 혹은 딥러닝 모델들을 학습 시킬 때 하이퍼 파라미터를 결정해야 하는 경우가 많이 있다. 이때 일반적인 하이퍼파리미터 튜닝 방법은 일저한 간격을 정해서 하이퍼파라미터 후보군을 설정하는 Grid Search와 일정 구간 내에서 랜덤하게 하이퍼 파리미터 후보군을 결정하는 Random Search가 활용된다. 하지만 두 방법론 모두 후보군에 대해서 성능을 확인한 후 비교해야 하므로 비효율적이다. 이러한 경우에 사용할 수 있는 것이 Bayesian Optimization이다. 베이지안 최적화는 크게 두 가지 핵심 모듈이 존재한다.</description>
    </item>
    
    <item>
      <title>Blending 기법</title>
      <link>http://sjh4773.github.io/post/machine-learning-12/</link>
      <pubDate>Wed, 28 Apr 2021 23:39:00 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-12/</guid>
      <description>예측값을 변수로 활용하는 앙상블 테크닉 Blending Blending 은 Ensemble 의 한 종류입니다. Ensemble 이란 예측 모형을 통합해서 하나의 예측을 수행하는 것을 말합니다. Ensemble 의 묘미는 서로 다른 예측 모형들을 합쳐 더 강한 예측 모형을 만들 수 있다는 것입니다. 가령 정확도 0.7, 0.7 인 모델 두 개를 합쳐서 0.9 을 만들 수 있습니다.
Blending 의 프로세스 1. Traning/Validation/Test set 을 나눈다. 2. Training set 에 모델 피팅을 한다. 3. Validation/Test set 에 대해 예측을 한다.</description>
    </item>
    
    <item>
      <title>pseudo labeling</title>
      <link>http://sjh4773.github.io/post/machine-learning-11/</link>
      <pubDate>Tue, 27 Apr 2021 22:05:28 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-11/</guid>
      <description>Pseudo Labeling   Pseudo Labeling이란? Labeled Data처럼 일일히 label을 하기보다, 이미 가지고 있는 Labeled data에 기반하여 대략적인 Labled을 주는 것
  Pseudo Labeling의 순서
  Labeled Data로 Model을 먼저 학습시킨다.
  그렇게 학습된 모델을 사용하여, Unlabeled Data를 예측하고 그 결과를 Label로 사용하는 Pseudo-labeled data를 만든다.
  Pseudo-labeled data와 Labeled를 모두 사용하여 다시 그 모델을 학습시킨다.
    Pseudo Labeled data  Pseudo Label은 아래와 같은 식으로, 각각의 sample에 대해, 예측된 확률이 가장 높은 것으로 정합니다.</description>
    </item>
    
    <item>
      <title>기본 스태킹 및 교차 검증 스태킹</title>
      <link>http://sjh4773.github.io/post/machine-learning-10/</link>
      <pubDate>Sun, 25 Apr 2021 21:36:52 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-10/</guid>
      <description>Basic Stacking Model - Diagram 기반 모델들이 예측한 값들을 Stacking 형태로 만들어서 메타 모델이 이를 학습하고 예측하는 모델
교차 검증 세트 기반의 스태킹 교차 검증 세트 기반의 스태킹은 이에 대한 개선을 위해 개별 모델들이 각각 교차 검증으로 메타 모델을 위한 학습용 스태킹 데이터 생성과 예측을 위한 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측을 수행합니다. 이는 다음과 같이 2단계의 스텝으로 구분될 수 있습니다.    스텝 1 : 각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한/학습용 테스트용 데이터를 생성합니다.</description>
    </item>
    
    <item>
      <title>Bayesian Optimization</title>
      <link>http://sjh4773.github.io/post/machine-learning-9/</link>
      <pubDate>Tue, 20 Apr 2021 00:42:51 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-9/</guid>
      <description>만약 우리에게 모델이 있는데, 미지의 함수(black-box function)라 예측은 잘 되는데 그 속을 들여다 볼 수 없다. 즉, f(x)가 closed-form으로 표현되지 않는다.(닫힌 형태(closed form)란 방정식(equation)의 해(solution)를 해석적(analytic)으로 표현할 수 있는 종류의 문제를 말한다.) 또 non-linear(비선형) + non-convex 해서 gradient도 찾을 수 없다.  Non-Convex한 함수란, 볼록함수와는 달리 극점이 굉장히 많을 수 있는 함수이기 때문에, Gradient Descent를 통해 구한 최솟값이 Local Minumum일 뿐, Global Minimum값은 아닐 수 있는 가능성이 생긴다. 심지어 query를 입력으로 주고 evaluate 할 때 엄청난 시간/비용이 든다.</description>
    </item>
    
    <item>
      <title>머신러닝 회귀 모델의 평가지표</title>
      <link>http://sjh4773.github.io/post/machine-learning-8/</link>
      <pubDate>Thu, 15 Apr 2021 22:37:09 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-8/</guid>
      <description>회귀 평가 지표 회귀 평가를 위한 지표는 실제 값과 예측값의 차이를 기반으로 함
회귀 평가지표 MAE, MSE, RMSE, RMSLE는 값이 작을수록 회귀 성능이 좋은 것 값이 작을수록 예측값과 실제값의 차이가 없다는 것을 의미
MSE(Mean Squared Error) 실제 값과 예측 값의 차이를 제곱해 평균한 것
장점  지표 자체가 직관적이고 단순하다.  단점 스케일에 의존적이다. 예를 들어 테슬라의 주가가 900000원이고 현대자동차의 주가가 250000일 때, 두 주가를 예측하는 각각의 모델의 MSE가 똑같이 4000이 나올 경우, 분명 동일한 에러율이 아님에도 불구하고 동일하게 보여진다.</description>
    </item>
    
    <item>
      <title>logloss</title>
      <link>http://sjh4773.github.io/post/machine-learning-7/</link>
      <pubDate>Tue, 13 Apr 2021 23:06:12 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-7/</guid>
      <description>Log Loss란?  모델 성능 평가 시 사용가능한 지표 분류 모델 평가 시 사용 확률값을 기준으로 평가  예를 들어 오지선다 객관식 문제를 국진학생과 성진학생이 풀고 있다고 가정한다. 국진 학생은 구구단을 열심히 외운 학생입니다 그래서 문제를 보자마자 99%확률로 정답은 1번 15이다라고 판단하고 1번을 고릅니다. 반면 성진학생은 아직 구구단을 다외우지 못했습니다 그래서 20% 확률로 정답을 찍기로 결심합니다 그런데 얼떨결에 1번을 찍게 되어서 문제를 맞추게 되었습니다. 단순히 정답을 맞추는 여부로 두 학생을 평가하게 된다면 두 학생은 문제를 맞추었으므로 국진 학생과 성진학생이 실력이 같다고 평가 받게 됩니다.</description>
    </item>
    
    <item>
      <title>GAN 이론</title>
      <link>http://sjh4773.github.io/post/machine-learning-6/</link>
      <pubDate>Sun, 04 Apr 2021 23:25:57 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-6/</guid>
      <description>GAN이란? GAN은 &amp;lsquo;생성적 적대 신경망&amp;rsquo;의 약자로 풀어서 쓰면, 생성자와 식별자가 서로 경쟁(Adversarial)하며 데이터를 생성(Generative)하는 모델(Network)을 뜻한다.
잡음의 이미지로부터 무엇인가를 생성해낸 결과에 대해서 분류기라고 하는 Dis가 G(Z)를 가짜라고 분류함 반면에 원래의 이미지를 받아왔을 때 원래 이미지에 대해서는 Positive하게 반응함 원래 이미지에 대해서는 확률 높게 판단해주고 가짜 이미지에 대해서는 확신도를 아주 낮게 준다.   잡음 이미지 Z 생성신경망 통과 Generator 생성 이미지 획득 G(Z) 원 이미지 X 분류기 통과 Discriminator(원래 이미지와 생성기로부터 만들어진 이미지를 동시에 입력받음) 두 분류 결과 (1:생성이미지 D(G(Z)), 2:원래이미지 D(X)) 1번에 대해서는 가짜, 2번에 대해서는 진짜로 분류하게끔 학습 목표를 제공해주어야 하는 것이 알고리즘의 총흐름도 이전 6단계 결과에 대한 적대적 학습시행 -&amp;gt; 생성이미지 출신 저하(생성이미지가 가짜다) &amp;amp; 원래이미지 출신 상승(원래이미지에 대한 확률을 높게 평가한다)  Gradient descending 경사하강법을 이용하면 어떤 지점으로 수렴하게 되는 결과를 받아들임 이 함수의 곡선을 가지고 Gradient ascending을 한다고 가정하면 바깥 영역에서 함수들이 굉장히 커지게 되며 그 끝을 알 수 없기 때문에 학습이 엉망으로 진행된다.</description>
    </item>
    
    <item>
      <title>데이터 전처리</title>
      <link>http://sjh4773.github.io/post/machine-learning-5/</link>
      <pubDate>Sat, 03 Apr 2021 23:46:27 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-5/</guid>
      <description>데이터 전처리   데이터 클렌징
  결손값 처리(Null/NaN 처리)
  데이터 인코딩(레이블, 원-핫 인코딩)
  머신러닝 알고리즘은 문자열 데이터 속성을 입력 받지 않으며 모든 데이터는 숫자형으로 표현되어야 합니다. 문자형 카테고리형 속성은 모두 숫자값으로 변환/인코딩 되어야 합니다.
레이블(Label) 인코딩  [TV, 냉장고, 전자레인지, 컴퓨터, 선풍기, 믹서] -&amp;gt; [0, 1, 4, 5, 3, 2] 문제점 : 분류를 레이블 인코딩하면 관계성이 확보되지 않는다.  원-핫(One-Hot) 인코딩 원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 컬럼에만 1을 표시하고 나머지 컬럼에는 0을 표시하는 방식입니다.</description>
    </item>
    
    <item>
      <title>Convolutional Neural Network를 이용한 MNIST</title>
      <link>http://sjh4773.github.io/post/machine-learning-4/</link>
      <pubDate>Sat, 03 Apr 2021 23:35:06 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-4/</guid>
      <description># 필요 라이브러리 호출 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets,transforms import numpy as np import matplotlib.pyplot as plt train_dataset = datasets.MNIST(&amp;#39;./mnist_data&amp;#39;, # torchvision 안에 있는 MNIST dataset을 불러옴  download=True, # 다운로드 할것인가? train=True, # train 용도 구분 transform=transforms.ToTensor() # torch를 입력 받을 수 있는 자료형태로 변환 ) test_dataset = datasets.MNIST(&amp;#39;./mnist_data&amp;#39;, download=False, train=False, transform=transforms.ToTensor() ) # 데이터셋 로더 호출 train_loader = torch.</description>
    </item>
    
    <item>
      <title>머신러닝 용어정리</title>
      <link>http://sjh4773.github.io/post/machine-learning-3/</link>
      <pubDate>Thu, 01 Apr 2021 23:46:51 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-3/</guid>
      <description>머신러닝을 위한 용어 정리 피처(Feature) = 속성  피처는 데이터 세트의 일반 속성임 머신러닝은 2차원 이상의 다차원 데이터에서도 많이 사용되므로 타겟값을 제외한 나머지 속성을 모두 피처로 지칭  레이블, 클래스, 타겟(값), 결정(값)  타겟값 또는 결정값은 지도 학습 시 데이터의 학습을 위해 주어지는 정답 데이터 지도 학습 중 분류의 경우에는 이 결정값을 레이블 또는 클래스로 지칭  지도학습이란? 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식입니다. 이 때 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트로 지칭합니다.</description>
    </item>
    
    <item>
      <title>이미지 경계선 검출</title>
      <link>http://sjh4773.github.io/post/machine-learning-2/</link>
      <pubDate>Sun, 28 Mar 2021 21:00:24 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning-2/</guid>
      <description># image convolution # check out horizontal, vertical, tilting, laplacian import numpy as np import matplotlib.pyplot as plt %matplotlib inline from PIL import Image # 파이썬에서 이미지를 담당 def convolution(image,filter,s=1): #stride = 1 image = np.asarray(image) height = image.shape[0] width = image.shape[1] new_height = int((height - filter.shape[0])/s+1) new_width = int((width - filter.shape[1])/s+1) new_image = np.zeros((new_height,new_width)) c_h = 0 for h in range(new_height): c_w = 0 for w in range(new_width): new_image[h,w] = np.</description>
    </item>
    
    <item>
      <title>mnist 데이터</title>
      <link>http://sjh4773.github.io/post/mnist/</link>
      <pubDate>Wed, 24 Mar 2021 21:21:29 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/mnist/</guid>
      <description>MNIST 데이터란 인공지능이 이미지를 보고 이것이 어떤 이미지인가를 맞춰보게 하는 인공지능 훈련을 위한 Dataset
MNIST 개요
 손글씨 데이터 손으로 작성된 0~9까지의 숫자 이미지 이미지 + 레이블 &amp;ndash;&amp;gt; 하나의 데이터셋  이미지 속성
 차원 (가로,세로,색조) &amp;ndash;&amp;gt; (28,28,1) 픽셀값: 0 ~ 255  레이블 정보
 단순 스칼라 (확률정보이용) -&amp;gt; One-hot encoding  해결전략
 이미지, 레이블 전처리  원본이미지 &amp;ndash;&amp;gt; 전처리(flatten)
레이블 &amp;ldquo;3&amp;rdquo; &amp;ndash;&amp;gt; &amp;ldquo;[0,0,0,1,0,0,0,0,0,0]&amp;rdquo; One-hot encoding
이미지 전처리</description>
    </item>
    
    <item>
      <title>회귀분석</title>
      <link>http://sjh4773.github.io/post/machine-learning/</link>
      <pubDate>Wed, 24 Mar 2021 21:09:29 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/machine-learning/</guid>
      <description>#Logistic regression example # 라이브러리 호출 import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import pandas as pd # 데이터를 가공하기 위함 import numpy as np import matplotlib.pyplot as plt # 그림을 그리기 위함 %matplotlib inline #파일 업로드 data = pd.read_csv(&amp;#39;./mba_admission.csv&amp;#39;) # ./ --&amp;gt; 현재 경로를 나타낸다 data.columns # 데이터 속성을 확인함 data.shape # (40,4) --&amp;gt; 행 : 데이터 크기, 열 : gmat, gpa, work_experience --&amp;gt; 입력, admitted --&amp;gt; 결과 data.</description>
    </item>
    
    <item>
      <title>파이토치를 이용한 데이터 가공 예시</title>
      <link>http://sjh4773.github.io/post/ai-2/</link>
      <pubDate>Tue, 23 Mar 2021 20:58:12 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/ai-2/</guid>
      <description>import torch import numpy as np import matplotlib.pyplot as plt %matplotlib inline # 데이터 x = np.random.uniform(-1.2, 2, size=(100)) #np.random.uniform은 Numpy에서 제공하는 균등분포 함수이다. y = 0.1*x**3-0.1*x**2+0.1*np.random.uniform(size=(100)) # 노이즈 추가 plt.plot(x,y,&amp;#39;o&amp;#39;) # 3차 함수지만 노이즈가 포함되어 있다. # train/validation/test 분류 data = np.concatenate((x.reshape(-1,1),y.reshape(-1,1)), axis=-1) # X, Y 묶음, X: input, Y: target print(data.shape) # 차원 0: 데이터 개수, 차원1: (x,y) --&amp;gt; (100,2) np.random.shuffle(data) # 데이터 골고루 섞음 test_num = int(data.shape[0]*0.1) # data.</description>
    </item>
    
    <item>
      <title>파이토치로 만드는 신경망</title>
      <link>http://sjh4773.github.io/post/ai/</link>
      <pubDate>Mon, 22 Mar 2021 20:58:12 +0000</pubDate>
      
      <guid>http://sjh4773.github.io/post/ai/</guid>
      <description># PyTorch 필요 라이브러리 호출 import torch # pip install pytorch 라이브러리 설치 import torch.nn as nn # 인공신경망을 구성해주는 메소드들의 집합 import torch.nn.functional as F # 활성화 함수들을 간단하게 호출 import torch.optim as optim # 토치를 통한 최적화 기능 # Neural network 모델 생성 # 입력 --&amp;gt; 은닉 (3,5) # 은닉 --&amp;gt; 출력 (5,2) # nn 뉴럴 네트워크 안에 있는 기능 하나를 호출  class fir_model(nn.Moudule): # nn.Moduel안에 있는 부모 클래스를 상속 받는다.</description>
    </item>
    
  </channel>
</rss>
