<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Lstm 이론 - My New Hugo Site</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Lstm 이론" />
<meta property="og:description" content="딥러닝은 학습하는데 있어서 주로 gradient descent를 사용하고 RNN 역시 gradient descent 사용
gradient desent 과정을 통해서 에러를 미분한 값은 E1의 에러값을 미분한 값, E2를 미분한 값, E3를 미분한 값과 같음
여기서 E3를 미분한 값을 보면 backpropagation through time을 통해서 여러 개의 미분값을 곱하게 되는데
짧은 시퀀스 같은 경우 큰 문제가 없음 하지만 긴 시퀀스 같은 경우 예를 들어 100개 이상의 단어가 있는 문장이 있으면 곱하기를 100회 수행하게 되는데 만약 미분 값이 1보다 작을 경우 새로운 weight value는 기존의 weight value와 거의 차이가 없다." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://sjh4773.github.io/post/lstm/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-05-28T18:14:17&#43;09:00" />
<meta property="article:modified_time" content="2021-05-28T18:14:17&#43;09:00" />


		<meta itemprop="name" content="Lstm 이론">
<meta itemprop="description" content="딥러닝은 학습하는데 있어서 주로 gradient descent를 사용하고 RNN 역시 gradient descent 사용
gradient desent 과정을 통해서 에러를 미분한 값은 E1의 에러값을 미분한 값, E2를 미분한 값, E3를 미분한 값과 같음
여기서 E3를 미분한 값을 보면 backpropagation through time을 통해서 여러 개의 미분값을 곱하게 되는데
짧은 시퀀스 같은 경우 큰 문제가 없음 하지만 긴 시퀀스 같은 경우 예를 들어 100개 이상의 단어가 있는 문장이 있으면 곱하기를 100회 수행하게 되는데 만약 미분 값이 1보다 작을 경우 새로운 weight value는 기존의 weight value와 거의 차이가 없다."><meta itemprop="datePublished" content="2021-05-28T18:14:17&#43;09:00" />
<meta itemprop="dateModified" content="2021-05-28T18:14:17&#43;09:00" />
<meta itemprop="wordCount" content="363">
<meta itemprop="keywords" content="Deep_learning," />
		<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Lstm 이론"/>
<meta name="twitter:description" content="딥러닝은 학습하는데 있어서 주로 gradient descent를 사용하고 RNN 역시 gradient descent 사용
gradient desent 과정을 통해서 에러를 미분한 값은 E1의 에러값을 미분한 값, E2를 미분한 값, E3를 미분한 값과 같음
여기서 E3를 미분한 값을 보면 backpropagation through time을 통해서 여러 개의 미분값을 곱하게 되는데
짧은 시퀀스 같은 경우 큰 문제가 없음 하지만 긴 시퀀스 같은 경우 예를 들어 100개 이상의 단어가 있는 문장이 있으면 곱하기를 100회 수행하게 되는데 만약 미분 값이 1보다 작을 경우 새로운 weight value는 기존의 weight value와 거의 차이가 없다."/>

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/custom.css">

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="My New Hugo Site" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">My New Hugo Site</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Lstm 이론</h1>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2 0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class="meta__text">JiHun</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2021-05-28T18:14:17&#43;09:00">2021-05-28</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/deep_learning/" rel="category">Deep_learning</a>
	</span>
</div></div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#lstm-메커니즘">LSTM 메커니즘</a>
      <ul>
        <li><a href="#lstm-잊는-메커니즘">LSTM 잊는 메커니즘</a></li>
        <li><a href="#lstm-새로운-정보를-추가하는-메커니즘">LSTM 새로운 정보를 추가하는 메커니즘</a></li>
        <li><a href="#lstm-출력">LSTM 출력</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<p><img src="/image/lstm1.PNG" alt=""></p>
<p><img src="/image/lstm2.PNG" alt=""></p>
<p>딥러닝은 학습하는데 있어서 주로 gradient descent를 사용하고  RNN 역시 gradient descent 사용</p>
<p><img src="/image/lstm3.PNG" alt=""></p>
<p>gradient desent 과정을 통해서 에러를 미분한 값은 E1의 에러값을 미분한 값, E2를 미분한 값, E3를 미분한 값과 같음</p>
<p><img src="/image/lstm4.PNG" alt=""></p>
<p>여기서 E3를 미분한 값을 보면
backpropagation through time을 통해서 여러 개의 미분값을 곱하게 되는데</p>
<p><img src="/image/lstm5.PNG" alt=""></p>
<pre><code>짧은 시퀀스 같은 경우 큰 문제가 없음
하지만 긴 시퀀스 같은 경우 예를 들어 100개 이상의 단어가 있는 문장이 있으면
곱하기를 100회 수행하게 되는데 만약 미분 값이 1보다 작을 경우 새로운 weight value는 기존의 weight value와
거의 차이가 없다. 즉 학습이 아무리 길어져도 weight value의 값이 거의 변하지 않음
이를 gradient vanishing 문제라고 합니다. 
</code></pre>
<p><img src="/image/lstm6.PNG" alt=""></p>
<pre><code>모든 미분 값이 1보다 클 경우에 이를 100번 이상 곱한다고 생각하면 상당히 값이 커질것이라고 생각할 수 있음
즉, 새로운 weight value는 기존의 weight value와 상당히 달라지게 되어있음 weight value의 변화폭이 크게 되며
학습의 방향이 한 곳으로 향하지 못하게 된다. 이를 gradient exploding 문제라고 합니다.
</code></pre>
<p>이러한 문제 때문에 시퀀스가 길어질 경우 RNN이 비효율적이다 이러한 문제를 해결하기 위해서 나온것이 LSTM이다.</p>
<p><img src="/image/lstm7.PNG" alt=""></p>
<p>이와 같은 경우에 주어를 선택해야할 경우 memory cell은 john에 대한 정보를 계속 간직한 상태로 he라는 정보를 출력해야함</p>
<p><img src="/image/lstm8.PNG" alt=""></p>
<p>이와 같은 경우에 주어를 선택해야할 경우 memory cell은 john에 대해 잊고 jane에 대한 정보를 기억해야함.</p>
<p><img src="/image/lstm10.PNG" alt=""></p>
<p>어떠한 정보를 잊는 메커니즘 어떠한 정보를 기억하는 메커니즘 이러한 것들이 LSTM 셀에 들어 있음.</p>
<p><img src="/image/lstm9.PNG" alt=""></p>
<p>LSTM cell의 가장 간단한 모양이고 y 출력이 있으며 memory cell과 hidden state가 옆의 셀로 전파가 된다는 것을 알 수 있음</p>
<h2 id="lstm-메커니즘">LSTM 메커니즘</h2>
<h3 id="lstm-잊는-메커니즘">LSTM 잊는 메커니즘</h3>
<p><img src="/image/lstm11.PNG" alt=""></p>
<pre><code>과거의 문장에 대한 정보 Ct-1(John에 대한 정보)
Jane 이라는 새로운 문장이 들어왔을 때 과거 은닉상태와 함께 시그모이드에 들어가게 된다.
이때 시그모이드의 출력값은 0~1로써 확률값이라고 이야기 할 수 있다.
만약 sigmoid 결과 20%라는 값이 나왔을 경우에 이것이 과거의 정보(Ct-1)와 곱해진 결과의 해석은
새로운 정보인 Jane이 들어왔을 때 과거의 정보를 20%만 간직하라는 의미이다.

학습과정을 통해서 forget mechanism의 weight value와 bias value가 optimize됨.
</code></pre>
<h3 id="lstm-새로운-정보를-추가하는-메커니즘">LSTM 새로운 정보를 추가하는 메커니즘</h3>
<p><img src="/image/lstm12.PNG" alt=""></p>
<pre><code>과거의 정보를 어느정도 잊었고 새로운 정보를 메모리셀에 추가해주는 부분.
 
과거 은닉상태와 현재 input이 들어와서 sigmoid 와 tanh가 곱해진다.
새로운 정보가 메모리셀에 더해지는 과정

학습과정을 통해서 input mechanism의 weight value와 bias value가 optimize됨.
</code></pre>
<h3 id="lstm-출력">LSTM 출력</h3>
<p><img src="/image/lstm13.PNG" alt=""></p>
<pre><code>정보를 출력하고 state를 다음 셀에 전달한다.
메모리셀에 있는 정보가 tanh를 통해 들어오고 그리고 은닉상태와 현재의 정보가 시그모이드를 통해 들어와서
서로 곱한다. 이 곱해진 값이 output을 출력이 되고 이것이 다음 state로 전달된다.
</code></pre>
<p>출처 : [딥러닝] LSTM 쉽게 이해하기,https://www.youtube.com/watch?v=bX6GLbpw-A4</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/deep_learning/" rel="tag">Deep_learning</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<div class="authorbox__header">
		<span class="authorbox__name">About JiHun</span>
	</div>
	<div class="authorbox__description">
		John Doe&rsquo;s true identity is unknown. Maybe he is a successful blogger or writer. Nobody knows it.
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/post/r-8/" rel="prev">
			<span class="pager__subtitle">«&thinsp;Previous</span>
			<p class="pager__title">비정형 텍스트 마이닝 - 사용데이터 기생충</p>
		</a>
	</div>
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/post/titanic/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">정형 데이터 마이닝 - 타이타닉</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2021 JiHun.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="/js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>